# -*- coding: utf-8 -*-
"""IR_HW3_part3_genereate embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MJZKpSVcn8VsmAfKtK_8IPZ6IaCWpE5h
"""

import gdown

url = 'https://drive.google.com/uc?id=1moaGNpmZjjN0XH0dcTOybEz8NVS00x-6'
output = 'train_dataset.txt'
gdown.download(url, output, quiet=False)

url = 'https://drive.google.com/uc?id=19ajg9r-QMvDBuVDxBne4Y-vIzSqauAHS'
output = 'test_dataset.txt'
gdown.download(url, output, quiet=False)

import csv
from scipy.sparse import csr_matrix

num_records = 0
record = []
with open('train_dataset.txt', 'r', encoding='utf-8') as file:
    reader = csv.reader(file, delimiter='\t', quoting=csv.QUOTE_ALL, quotechar='"')
    next(reader) # Skip the first line
    for row in reader:
        if len(row) == 4:

            record.append(row)

print(len(record))

# Get unique users and items
unique_users = list(set([row[0] for row in record]))
unique_items = list(set([row[1] for row in record]))

# Create mappings from user_id and item_id to row and column indices
user_mapping = {user: index for index, user in enumerate(unique_users)}
item_mapping = {item: index for index, item in enumerate(unique_items)}

# Map user_id and item_id to row and column indices
record = [[user_mapping[row[0]], item_mapping[row[1]], row[2], row[3]] for row in record]

# Separate the ratings and review texts
ratings = [float(row[2]) for row in record]

# Create a sparse matrix for the ratings
r = csr_matrix((ratings, ([record[i][0] for i in range(len(record))], [record[i][1] for i in range(len(record))])))
R_matrix = r.toarray()
print(f'Number of records: {num_records}')
print(f'R matrix: \n{R_matrix}')

print(r)
print(R_matrix.size)

print(len(unique_users))
print(len(unique_items))

print(len(record))

display(R_matrix)

"""**Load  BERT model**"""

from transformers import BertModel, BertTokenizer
import torch


model_name = 'bert-base-uncased'

# Load the pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

"""**:**

"""

#Extract the review texts from the record

reviews = [row[3] for row in record]

"""# **Generate embeddings**"""

from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = model.to(device)

item_ids = [row[1] for row in record]
item_embeddings = {}
for item_id in tqdm(set(item_ids)):
    # Get the indices of the reviews that belong to the current item
    review_indices = [i for i, id in enumerate(item_ids) if id == item_id]
    # Get the review texts that belong to the current item
    item_reviews = [reviews[i] for i in review_indices]
    # Tokenize the review texts and convert them to tensors
    inputs = tokenizer(item_reviews, return_tensors='pt', padding=True, truncation=True).to(device)

    with torch.no_grad():
        outputs = model(**inputs)


    # embedding for each review
    review_embeddings  = outputs.last_hidden_state.mean(dim=1)

    # average of the embeddings for the current item
    item_embeddings[item_id] = review_embeddings.mean(dim=0)
    del inputs, outputs
    torch.cuda.empty_cache()

# Save the item embeddings for next parts

torch.save(item_embeddings, 'item_embeddings.pt')